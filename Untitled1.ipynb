{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fd0a6-dd8a-4a0a-880c-ff4baa774af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MODEL PART\"\"\"\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11s.pt\")\n",
    "\n",
    "# Train the model\n",
    "train_results = model.train(\n",
    "    data=\"C:/Users/Slay/Desktop/data.yaml\",  # path to dataset YAML\n",
    "    epochs=200,  # number of training epochs\n",
    "    imgsz=620,  # training image size\n",
    "    device=\"cpu\",\n",
    "    batch=128,\n",
    "    workers=5,\n",
    ")\n",
    "\n",
    "# Evaluate model performance on the validation set\n",
    "\n",
    "\n",
    "# Perform object detection on an imageW\n",
    "results = model(\"C:/Users/Slay/Desktop/minecraftHT/val/images/688595035175screenshot.png\")\n",
    "results = model(\"C:/Users/Slay/Desktop/minecraftHT/val/images/688595035175screenshot.png\")\n",
    "results = model(\"C:/Users/Slay/Desktop/minecraftHT/val/images/688595035175screenshot.png\")\n",
    "results[0].show()\n",
    "detections = results[0].boxes\n",
    "\n",
    "\n",
    "os.remove(\"trained_model.pt\")\n",
    "model.save(\"trained_model.pt\")\n",
    "first_detection = detections.xyxy[0]  # Координаты bounding box\n",
    "first_class = detections.cls[0]  # Класс первого объекта\n",
    "first_conf = detections.conf[0]  # Уверенность первого объекта\n",
    "    \n",
    "    # Выводим информацию о первом обнаруженном объекте\n",
    "print(f\"Класс: {first_class}, Уверенность: {first_conf}, Координаты: {first_detection}\")\n",
    "\n",
    "    # Отображаем изображение с первым bounding box\n",
    "\n",
    "\n",
    "print(\"Объекты не обнаружены.\")\n",
    "\n",
    "model = YOLO(\"trained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d5e6b3-317e-4aec-a54e-faaa9175f43d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyautogui' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     17\u001b[0m mouses \u001b[38;5;241m=\u001b[39m Controller()\n\u001b[1;32m---> 18\u001b[0m lenght, height\u001b[38;5;241m=\u001b[39m \u001b[43mpyautogui\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m     19\u001b[0m x_center, y_center \u001b[38;5;241m=\u001b[39m lenght\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1.9375\u001b[39m, height\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1.9375\u001b[39m\n\u001b[0;32m     20\u001b[0m left\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x_center\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m640\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pyautogui' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"AIM BOT PART\"\"\"\n",
    "import pyautogui\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import threading\n",
    "import time\n",
    "import numpy as np\n",
    "from pynput.mouse import Controller\n",
    "import keyboard\n",
    "time.sleep(3)\n",
    "mouses = Controller()\n",
    "lenght, height= pyautogui.size()\n",
    "x_center, y_center = lenght/1.9375, height/1.9375\n",
    "left= int(x_center-640)\n",
    "top=int(y_center-640)\n",
    "width=1240\n",
    "height=1240\n",
    "mouse = Controller()\n",
    "def mousetp():\n",
    " mouse.position = (x+660, y-100) \n",
    "\n",
    "\n",
    "\n",
    "for i in range(500):\n",
    " screenshot = pyautogui.screenshot(region=(left, top, width, height))\n",
    "\n",
    "# Преобразуем скриншот в массив NumPy\n",
    " screenshot_np = np.array(screenshot)\n",
    "\n",
    "# Изменяем размер изображения до 640x640 пикселей\n",
    " resized_image = Image.fromarray(screenshot_np).resize((640, 640), Image.LANCZOS)\n",
    "\n",
    " # Сохраняем измененное изображение\n",
    " image_tensor = transforms.ToTensor()(resized_image).unsqueeze(0)  # Добавляем размерность для батча\n",
    "\n",
    "    # Выполняем предсказания\n",
    "  \n",
    "\n",
    "\n",
    " results = model.predict(source=image_tensor)\n",
    "\n",
    " detections = results[0].boxes\n",
    "    \n",
    "\n",
    "\n",
    " if detections.shape[0] > 0:  # Проверяем, есть ли детекции\n",
    "    conf =detections[0].conf.item()\n",
    "    if conf>0.80:\n",
    "     print(conf)\n",
    "     boxes = detections.xyxy[0]  # Получаем боксы\n",
    "     x = (boxes[0] + boxes[2])\n",
    "     y = (boxes[1] + boxes[3]) \n",
    "     my_thread = threading.Thread(target=mousetp)\n",
    "     my_thread.start()\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba621c-502f-44ac-859a-cca65325350e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
